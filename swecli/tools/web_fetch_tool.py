"""Web fetching tool for retrieving content from URLs using Crawl4AI."""

from __future__ import annotations

import asyncio
import signal
import threading
from pathlib import Path
from typing import Any, Optional, Sequence

from crawl4ai import AsyncWebCrawler, BrowserConfig, CacheMode, CrawlerRunConfig
from crawl4ai.deep_crawling import (
    BFSDeepCrawlStrategy,
    BestFirstCrawlingStrategy,
    DFSDeepCrawlStrategy,
)
from crawl4ai.deep_crawling.filters import DomainFilter, FilterChain, URLPatternFilter

from swecli.models.config import AppConfig


class WebFetchTool:
    """Tool for fetching web content using Crawl4AI."""

    def __init__(self, config: AppConfig, working_dir: Path):
        """Initialize web fetch tool.

        Args:
            config: Application configuration
            working_dir: Working directory (not used but kept for consistency)
        """
        self.config = config
        self.working_dir = working_dir
        self.timeout = 30000  # 30 second timeout for page load

    def fetch_url(
        self,
        url: str,
        extract_text: bool = True,
        max_length: Optional[int] = 50000,
        deep_crawl: bool = False,
        crawl_strategy: str = "best_first",
        max_depth: int = 1,
        include_external: bool = False,
        max_pages: Optional[int] = None,
        allowed_domains: Optional[Sequence[str]] = None,
        blocked_domains: Optional[Sequence[str]] = None,
        url_patterns: Optional[Sequence[str]] = None,
        stream: bool = False,
        task_monitor: Optional[Any] = None,
    ) -> dict[str, any]:
        """Fetch content from a URL using Crawl4AI.

        Args:
            url: URL to fetch
            extract_text: If True, return markdown format. If False, return raw HTML
            max_length: Maximum content length (None for no limit)
            deep_crawl: If True, follow links using a deep crawl strategy
            crawl_strategy: One of bfs, dfs, best_first (best_first default)
            max_depth: Maximum depth to traverse when deep crawling
            include_external: Allow following external domains
            max_pages: Optional cap on total crawled pages
            allowed_domains: Optional list of domains to keep during deep crawl
            blocked_domains: Optional list of domains to skip during deep crawl
            url_patterns: Optional glob patterns to include
            stream: If True (and deep_crawl), stream results as they are discovered
            task_monitor: Optional task monitor for checking interrupts

        Returns:
            Dictionary with success, content, and optional error
        """
        # Check for interrupt before starting fetch
        if task_monitor and task_monitor.should_interrupt():
            return {
                "success": False,
                "error": "Operation cancelled by user",
                "content": None,
                "interrupted": True,
            }

        # Use threading and asyncio cancellation for immediate interrupt capability
        result_container = {}
        exception_container = {}

        def run_async_in_thread():
            """Run the async operation in a separate thread."""
            try:
                # Create new event loop for this thread
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)

                # Create a task that can be cancelled
                task = new_loop.create_task(
                    self._fetch_url_async(
                        url=url,
                        extract_text=extract_text,
                        max_length=max_length,
                        deep_crawl=deep_crawl,
                        crawl_strategy=crawl_strategy,
                        max_depth=max_depth,
                        include_external=include_external,
                        max_pages=max_pages,
                        allowed_domains=allowed_domains,
                        blocked_domains=blocked_domains,
                        url_patterns=url_patterns,
                        stream=stream,
                        task_monitor=task_monitor,
                        cancel_event=cancel_event,
                    )
                )

                # Store the task so it can be cancelled
                result_container['task'] = task

                # Run the task
                result = new_loop.run_until_complete(task)
                result_container['result'] = result

            except asyncio.CancelledError:
                result_container['result'] = {
                    "success": False,
                    "error": "Operation cancelled by user",
                    "content": None,
                    "interrupted": True,
                }
            except Exception as e:
                exception_container['exception'] = e
            finally:
                new_loop.close()

        # Create cancellation event
        cancel_event = threading.Event()

        # Start the operation in a background thread
        worker_thread = threading.Thread(target=run_async_in_thread)
        worker_thread.daemon = True
        worker_thread.start()

        # Monitor for interrupts while the operation is running
        while worker_thread.is_alive():
            # Check for user interrupt
            if task_monitor and task_monitor.should_interrupt():
                # Signal cancellation
                cancel_event.set()

                # Cancel the asyncio task if it was created
                if 'task' in result_container and result_container['task']:
                    try:
                        # Call cancel in the event loop thread
                        loop = result_container['task'].get_loop()
                        if loop and not loop.is_closed():
                            loop.call_soon_threadsafe(result_container['task'].cancel)
                    except:
                        pass

                # Wait for thread to finish with timeout
                worker_thread.join(timeout=1.0)

                # Return interrupt result
                return {
                    "success": False,
                    "error": "Operation cancelled by user",
                    "content": None,
                    "interrupted": True,
                }

            # Small delay to prevent busy waiting
            worker_thread.join(timeout=0.1)

        # Check if the thread completed successfully
        if exception_container:
            raise exception_container['exception']

        if 'result' in result_container:
            return result_container['result']

        # Fallback (shouldn't reach here)
        return {
            "success": False,
            "error": "Unknown error during web fetch",
            "content": None,
        }

    async def _fetch_url_async(
        self,
        url: str,
        extract_text: bool = True,
        max_length: Optional[int] = 50000,
        deep_crawl: bool = False,
        crawl_strategy: str = "best_first",
        max_depth: int = 1,
        include_external: bool = False,
        max_pages: Optional[int] = None,
        allowed_domains: Optional[Sequence[str]] = None,
        blocked_domains: Optional[Sequence[str]] = None,
        url_patterns: Optional[Sequence[str]] = None,
        stream: bool = False,
        task_monitor: Optional[Any] = None,
        cancel_event: Optional[threading.Event] = None,
    ) -> dict[str, any]:
        """Async implementation of URL fetching.

        Args:
            url: URL to fetch
            extract_text: If True, return markdown format. If False, return raw HTML
            max_length: Maximum content length (None for no limit)
            deep_crawl: If True, follow links using a deep crawl strategy
            crawl_strategy: One of bfs, dfs, best_first (best_first default)
            max_depth: Maximum depth to traverse when deep crawling
            include_external: Allow following external domains
            max_pages: Optional cap on total crawled pages
            allowed_domains: Optional list of domains to keep during deep crawl
            blocked_domains: Optional list of domains to skip during deep crawl
            url_patterns: Optional glob patterns to include
            stream: If True (and deep_crawl), stream results as they are discovered

        Returns:
            Dictionary with success, content, and optional error
        """
        try:
            # Validate URL format
            if not url.startswith(("http://", "https://")):
                return {
                    "success": False,
                    "error": f"Invalid URL: must start with http:// or https://",
                    "content": None,
                }

            # Skip file downloads that cause navigation errors
            file_extensions = ('.zip', '.pdf', '.exe', '.dmg', '.tar.gz', '.rar', '.mp4', '.avi', '.mov', '.iso', '.pkg')
            if any(url.lower().endswith(ext) for ext in file_extensions):
                return {
                    "success": False,
                    "error": f"File downloads are not supported: {url}",
                    "content": None,
                }

            if deep_crawl and max_depth < 1:
                return {
                    "success": False,
                    "error": "max_depth must be greater than 0 when deep_crawl is enabled",
                    "content": None,
                }

            if deep_crawl and max_pages is not None and max_pages < 1:
                return {
                    "success": False,
                    "error": "max_pages must be greater than 0 when specified",
                    "content": None,
                }

            filter_chain = None
            deep_strategy = None
            stream_mode = stream if deep_crawl else False

            if deep_crawl:
                filter_chain = self._build_filter_chain(allowed_domains, blocked_domains, url_patterns)
                deep_strategy = self._build_deep_strategy(
                    strategy=crawl_strategy,
                    max_depth=max_depth,
                    include_external=include_external,
                    max_pages=max_pages,
                    filter_chain=filter_chain,
                )
            else:
                filter_chain = None

            # Configure browser
            browser_config = BrowserConfig(
                headless=True,
                verbose=False,
                user_agent="SWE-CLI/1.0 (AI Assistant Tool; Crawl4AI)",
            )

            # Configure crawler
            run_config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                word_count_threshold=10,
                exclude_external_links=False,
                remove_overlay_elements=True,
                process_iframes=False,
                page_timeout=self.timeout,
                deep_crawl_strategy=deep_strategy,
                stream=stream_mode,
            )

            # Check for interrupt before starting web crawler
            if task_monitor and task_monitor.should_interrupt() or (cancel_event and cancel_event.is_set()):
                return {
                    "success": False,
                    "error": "Operation cancelled by user",
                    "content": None,
                    "interrupted": True,
                }

            # Fetch URL
            async with AsyncWebCrawler(config=browser_config) as crawler:
                # Check for interrupt one more time before actual fetch
                if task_monitor and task_monitor.should_interrupt() or (cancel_event and cancel_event.is_set()):
                    return {
                        "success": False,
                        "error": "Operation cancelled by user",
                        "content": None,
                        "interrupted": True,
                    }

                if stream_mode:
                    page_results = []
                    async for streamed_result in crawler.arun(url=url, config=run_config):
                        # Check for interrupt during streaming
                        if task_monitor and task_monitor.should_interrupt() or (cancel_event and cancel_event.is_set()):
                            return {
                                "success": False,
                                "error": "Operation cancelled by user",
                                "content": None,
                                "interrupted": True,
                            }
                        page_results.append(streamed_result)
                else:
                    # Use a timeout wrapper to allow periodic cancellation checks
                    try:
                        result = await asyncio.wait_for(
                            crawler.arun(url=url, config=run_config),
                            timeout=1.0  # Check for cancellation every 1 second
                        )
                        page_results = result if isinstance(result, list) else [result]
                    except asyncio.TimeoutError:
                        # Check for cancellation on timeout
                        if task_monitor and task_monitor.should_interrupt() or (cancel_event and cancel_event.is_set()):
                            return {
                                "success": False,
                                "error": "Operation cancelled by user",
                                "content": None,
                                "interrupted": True,
                            }
                        # If not cancelled, continue with a longer timeout
                        result = await crawler.arun(url=url, config=run_config)
                        page_results = result if isinstance(result, list) else [result]

                if deep_crawl:
                    successful_pages = [
                        page for page in page_results if getattr(page, "success", True)
                    ]
                    if not successful_pages:
                        return {
                            "success": False,
                            "error": "Deep crawl completed but returned no successful pages",
                            "content": None,
                        }

                    content, page_metadata = self._format_deep_crawl_results(successful_pages, extract_text)
                    if max_length and len(content) > max_length:
                        content = (
                            content[:max_length]
                            + f"\n\n... (truncated, total length: {len(content)} characters)"
                        )

                    return {
                        "success": True,
                        "content": content,
                        "error": None,
                        "url": url,
                        "status_code": 200,
                        "content_type": "text/markdown" if extract_text else "text/html",
                        "pages": page_metadata,
                        "page_count": len(successful_pages),
                    }

                result = page_results[0]

                if not result.success:
                    error_msg = (
                        result.error_message if hasattr(result, "error_message") else "Unknown error"
                    )
                    return {
                        "success": False,
                        "error": error_msg,
                        "content": None,
                    }

                content = result.markdown if extract_text else result.html

                if max_length and len(content) > max_length:
                    content = (
                        content[:max_length]
                        + f"\n\n... (truncated, total length: {len(content)} characters)"
                    )

                return {
                    "success": True,
                    "content": content,
                    "error": None,
                    "url": result.url,  # Final URL after redirects
                    "status_code": 200,  # Crawl4AI doesn't expose status codes directly
                    "content_type": "text/markdown" if extract_text else "text/html",
                    "links": result.links if hasattr(result, "links") else {},
                    "media": result.media if hasattr(result, "media") else {},
                }

        except asyncio.TimeoutError:
            return {
                "success": False,
                "error": f"Request timeout after {self.timeout / 1000} seconds",
                "content": None,
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to fetch URL: {str(e)}",
                "content": None,
            }

    def _build_filter_chain(
        self,
        allowed_domains: Optional[Sequence[str]],
        blocked_domains: Optional[Sequence[str]],
        url_patterns: Optional[Sequence[str]],
    ) -> FilterChain | None:
        filters = []

        if allowed_domains or blocked_domains:
            filters.append(
                DomainFilter(
                    allowed_domains=list(allowed_domains) if allowed_domains else None,
                    blocked_domains=list(blocked_domains) if blocked_domains else None,
                )
            )

        if url_patterns:
            filters.append(URLPatternFilter(patterns=list(url_patterns)))

        return FilterChain(filters) if filters else None

    def _build_deep_strategy(
        self,
        *,
        strategy: str,
        max_depth: int,
        include_external: bool,
        max_pages: Optional[int],
        filter_chain: FilterChain | None,
    ):
        strategy_key = (strategy or "best_first").lower()
        strategy_map = {
            "bfs": BFSDeepCrawlStrategy,
            "dfs": DFSDeepCrawlStrategy,
            "best_first": BestFirstCrawlingStrategy,
            "best": BestFirstCrawlingStrategy,
        }

        if strategy_key not in strategy_map:
            raise ValueError(f"Unsupported crawl_strategy '{strategy}'. Use bfs, dfs, or best_first.")

        strategy_cls = strategy_map[strategy_key]
        kwargs: dict[str, Any] = {
            "max_depth": max_depth,
            "include_external": include_external,
        }

        if filter_chain:
            kwargs["filter_chain"] = filter_chain

        if max_pages is not None:
            kwargs["max_pages"] = max_pages

        return strategy_cls(**kwargs)

    def _format_deep_crawl_results(self, pages: Sequence[Any], extract_text: bool) -> tuple[str, list[dict[str, Any]]]:
        sections: list[str] = []
        metadata: list[dict[str, Any]] = []

        for index, page in enumerate(pages, start=1):
            page_url = getattr(page, "url", None)
            raw_metadata = getattr(page, "metadata", {}) or {}
            depth = raw_metadata.get("depth", 0) if isinstance(raw_metadata, dict) else raw_metadata

            page_content = page.markdown if extract_text else page.html
            page_content = page_content or ""

            sections.append(
                f"### Page {index}: {page_url or 'unknown'} (depth {depth})\n\n{page_content}".strip()
            )

            metadata.append(
                {
                    "url": page_url,
                    "depth": depth,
                    "content_length": len(page_content),
                }
            )

        combined_content = "\n\n".join(sections)
        return combined_content, metadata
